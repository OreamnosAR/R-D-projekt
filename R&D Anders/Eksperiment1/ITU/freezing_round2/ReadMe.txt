In addition to full fine-tuning, we used a freezing strategy
where we froze all the pre-trained weights to train the classification layer first
and then fine-tuned the whole network with the same hyperparameters as above.

omg...
